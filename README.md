# [docker-tutorial](https://docs.docker.com/get-started/part2/)
Gentle introduction to docker


# Containers


## First commands
```
docker --version

docker info
docker version

docker image ls --all
docker container ls --all
docker container ls -aq

docker run hello-world
```


## Building an image
The . specifies the current directory, omitting the :vxxxxx means that the image will be versioned at "latest" instead of "xxxxx"

```
docker build --tag=myImageName .
docker build --tag=myImageName:v1.0.1 .
```


## Running an image - Deploying the image as container
Image port 4000 on local host is mapped to port 80 on container.
app.py specified that the "server" would run at 0.0.0.0:80 which is mapped to our localhost:4000.
```
docker run -p 4000:80 friendlyhello_mytag
```
visit
```
http://localhost:4000
```
or execute
```
curl http://localhost:4000
```
Note that port 80 is first exposed when the container is setup in the docker file (**EXPOSE**).
Then a mapping can be done between the container's port 80 and localhost:4000 (**docker run -p**)

Also note that *myImageName* is the name of the image that generated the container.
Once the container is spawned it gets its own ID (similar to process id) and its own name.

Log in runninng container with
```
docker exec -it <container-name> sh
docker exec -it <container-name> bash
```
Also note that ***the app's host name is the container id***


## Pushing Image to Registry

```
A registry is a collection of repositories which are in turn a collection of images
An account can create many repositories (similar to github repositories)
```

```
docker login
```

The notation for associating a *local image* with a *repository* on a *registry* is ***username/repository:tag***. 
The tag is optional, but recommended, since it is the mechanism that registries use to give Docker images a version
```
docker tag myImageName kmurphs/get-started:part2
```

At this point we just tagged the image in a way that allows it to be pushed to the repository. However, at this point,
it is still an image like any other and can be viewed with:
```
docker image ls 
```

Once the tagging is done,
```
docker push username/repository:tag
```
Now it is visible in dockerhub


The following will pull the specific image with the specified tag from dockerhub if the image is not found on your system
```
docker run -p 4000:80 username/repository:tag
```



# Services

The docker environment has 3 levels:
```
Stacks 
  (Swarms - Indirectly)
Services
Containers
```

We dealt with containers in the first part:
  **``A service is a construct built on top of containers that makes it possible to scale the application by 
  spawning more containers replica (generated by the same image) doing the same things but sharing the load and 
  increasing the service's processing power (horizontal scaling)
  ``**


### From the docs
> In a distributed application, different pieces of the app are called “services”. For example, if you imagine a video sharing site, it probably includes a service for storing application data in a database, a service for video transcoding in the background after a user uploads something, a service for the front-end, and so on.

> Services are really just “containers in production.” A service only runs one image, but it codifies the way that image runs—what ports it should use, how many replicas of the container should run so the service has the capacity it needs, and so on. Scaling a service changes the number of container instances running that piece of software, assigning more computing resources to the service in the process.


### docker-compose.yml

You configure and run the service by using the file ***``docker-compose.yml``***. See example below
```
version: "3"
services:
  web:
    # replace username/repo:tag with your name and image details
    image: username/repo:tag
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: "0.1"
          memory: 50M
      restart_policy:
        condition: on-failure
    ports:
      - "4000:80"
    networks:
      - webnet
networks:
  webnet:
```


0. **Create an app (stack) with services as listed, and networks as listed**
The service and network exist as addressable and identifiable entities

1. Create the services listed (In this case only one service is listed)
2. The service is **named** *web*
3. Pull the service's image from ...
4. Deployment policy:
    * 5 replicas at any given time
    * Limit cpu usage to 10% of one cpu for each replica
    * Allocate 50M RAM of memory to each replica
    * On failure, restart failed replica
5. Forward replica's internal port 80 to localhost's port 4000 via a load-balancing network called 
**webnet** defined below.
6. Since webnet does not have any specified parameters, it is configured with default parameters.


## Run the Service

```
docker swarm init
```

This instruction is explained later in part 4

```
docker stack deploy -c docker-compose.yml getstartedlab
```

The name ``getstartedlab`` is the name we give to our app/**stack**

List running services
```
docker service ls
```
List services in a stack
```
docker stack services <your app/stack name e.g. getstartedlab>
```
The name of a service is ``<your app/stack name e.g. getstartedlab>_<the network it's configured to use>`` e.g *getstartedlab_web*. 
A container within a service is called task and receives the name ``<service name>.<incrementing counter>``. 
The container still get listed with ``docker container ls``
```
docker service ps getstartedlab_web
curl -4 http://localhost:4000
```
Will convince of the load balancing, you will hit different containers as you curl into the docker app/stack


Rerun ``docker stack deploy -c docker-compose.yml getstartedlab`` with updated ``docker-compose.yml`` to update the deployment configuration.


## Stop the Service

Stop the app/stack
```docker stack rm getstartedlab```

Leave the swarm
```docker swarm leave --force```




# Swarms

Containers were replicated and integrated by a service. But so far they have been running on the same host/machine.
Swarms allow ***Multi-container, multi-machine applications***.

Note that *machine* here refers to either *physical* **or** *virtual* machines

A swarm (or swarm cluster) is therefore a ***"dockerized" cluster of machines*** (dockerized as opposed to other methods of joining machines together and still running Multi-container, multi-machine applications)

## Swarms Clusters
A group of physical and/or virtual machines that are running Docker and joined into a cluster.
One machine is selected to be the ***Swarm Manager*** who executes all the instructions we have used so far, and authorize new machines to join the cluster.
The other machines are called ***nodes/workers***. They only provide capacity and have no control or whatsoever influence over any other machines. 

```
docker swarm init
docker swarm join
```


  * ***Setup Windows 10 to run virtual machines***
      Run HyperV Manager, click Virtual Switch Manager, then Create Virtual Switch of type External with Sharing of active network adapter Enabled.
      
      ```
      docker-machine create -d hyperv --hyperv-virtual-switch "myswitch" myvm1
      docker-machine create -d hyperv --hyperv-virtual-switch "myswitch" myvm2
      ```


On Windows the terminal hanged at waiting for host to start. To fix this:
This [stack overlfow answer](https://stackoverflow.com/questions/47728330/docker-machine-stuck-while-creating) helped:
  * If you type: ``docker-machine ls``, you will see that the VM is running but they have no URL. This says that the network address cannot be assigned.
  * Use HyperV Manager to *turn off* the vms, and make sure they use an external switch adapter that's actually part of a **running** network (Wireless/Ethernet).
  * Use HyperV Manager to re-*start* the vms, and confirm that the issue is resolved by issuing another ``docker-machine ls`` in the terminal. The machine should have an IP address now.
  * **Note**: This [docker doc](https://docs.docker.com/machine/drivers/hyper-v/#example) suggest to reboot after setting up the switch adapter.

At this point, another error was generated on my windows system.
The ``docker-machine ls`` showed that even though the IP address were set properly, the following errors showed up:
```
    Unable to query docker version: Get https://192.168.8.104:2376/v1.15/version: x509: certificate has expired or is not yet valid.   
    Unable to query docker version: Get https://192.168.8.104:2376/v1.15/version: x509: certificate signed by unknown authority
```


This [issue thread](https://github.com/sparkfabrik/sparkdock/issues/14) and [this docker issue thread](https://github.com/docker/machine/issues/4046) helped. They also reference [this docker doc]( https://docs.docker.com/machine/reference/regenerate-certs/).

In the terminal,
```
docker-machine regenerate-certs [your vm name]
```

After this, ``docker-machine ls`` showed the vms up and running and without errors.


### Initialize the swarm and add nodes

The first nodes will always be the manager:
```
docker-machine ssh myvm1 "docker swarm init --advertise-addr <myvm1 ip>:<The daemon port, usually 2377>"
```
The response of this command gives instructions on how to add another node/worker or node/manager
```
docker-machine ssh myvm2 "<content of the response that corresponds to the type of node to be added>"
```
To leave the swarm
```
docker-machine ssh <relevant machine> "docker swarm leave"
docker-machine ssh <node manager machine> "docker swarm leave --force"
```


### Communicating with the nodes

So far, we have used:
```
docker-machine ssh <relevant machine>
```
But alternatively, the command below can also be used:
```
docker-machine env <relevant machine>
```
It configures the current terminal to talk directly to ``<the relevant machine>``.
The last line of the response to this command specifies what has to be done in order for the configuration to be finalized. In my case:
```
@FOR /f "tokens=*" %i IN ('docker-machine env myvm1') DO @%i
```

Then ``docker-machine ls`` will indicate to which machine the terminal is directly connected. Look at the asterisk in the ACTIVE column next to the ``<relevant machine>``.


### Deploying the app/stack
From a terminal configured to talk directly to the node manager (If this is not the case, use a bash command line to transfer the *docker-compose.yml* file to the node manager with **``docker-machine scp <file> <machine>:~``**)
```
docker stack deploy -c docker-compose.yml getstartedlab
```

``docker stack ps getstartedlab`` will give you the deployment status of your app/stack


### Accessing the cluster
The [docker doc](https://docs.docker.com/get-started/part4/) states that the following should allow you to hit the app endpoints:
```
http://<node ip address>  -  from the browser
curl http://<node ip address>  - from the terminal
```


However, it did not: 

``curl http://192.168.8.104:2376 --output -`` and ``curl http://192.168.8.105:2376 --output -`` gave ``§♥☺ ☻☻`` 

where the ip address are the ip of my virtual machines. The port 2376 is the port of the daemon running on the machine.
Ping were going through: ``ping 192.168.8.104`` and ``ping 192.168.8.105``


After running, 
```
docker service inspect getstartedlab_web
```
where *getstartedlab_web* is the service created by ``docker-compose.yml``, it was noticed that the ``"PublishMode": "ingress"`` was implemented at ``"PublishedPort": 4000,`` under the ``Endpoint.Ports`` section of the terminal response.

then,
```
http://<node ip address>:<ingress port>  -  from the browser
curl http://<node ip address>:<ingress port>  - from the terminal
```
allowed me to hit the app's endpoints.


**Note**: For each execution of the above 2 commands, the host name is different (We are hitting different containers - **horizontal scaling was successful*). One can verify that the ***host names*** always belong to the ***container ids*** as given by:
```
docker-machine ssh myvm1 "docker container ls"
docker-machine ssh myvm2 "docker container ls"
```

### Killing the stack and swarm, Restarting the swarm
This kills the stack:
```
docker stack rm getstartedlab
```

This kills the swarm:
```
docker-machine ssh <worker machine> "docker swarm leave"
docker-machine ssh <manager machine> "docker swarm leave --force"
```

Instead of killing the swarm, it can be stop and restarted
```
docker-machine stop <workers machine-name>
docker-machine stop <manager machine-name>


docker-machine start <manager machine-name>
docker-machine start <workers machine-name>
```



# Stacks

So far, although swarms allow multi-containers, multi-machines application, we have only used one service stretched over 5 containers and 2 machines.
A stack allows deployment of ***multi-containers, multi-machines, multi-services***
From the docs:
> A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together. A single stack is capable of defining and coordinating the functionality of an entire application (though very complex applications may want to use multiple stacks).


## 2 New Services

We are going to add a visualizer service and a redis database for data persistence.

``docker-compose.yml`` becomes:
```
version: "3"
services:
  web:
    # replace username/repo:tag with your name and image details
    image: username/repo:tag
    deploy:
      replicas: 5
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.1"
          memory: 50M
    ports:
      - "80:80"
    networks:
      - webnet
  visualizer:
    image: dockersamples/visualizer:stable
    ports:
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints: [node.role == manager]
    networks:
      - webnet
  redis:
    image: redis
    ports:
      - "6379:6379"
    volumes:
      - "/home/docker/data:/data"
    deploy:
      placement:
        constraints: [node.role == manager]
    command: redis-server --appendonly yes
    networks:
      - webnet
networks:
  webnet:
```

Two new services are added:
1. A **visualizer Service** based on the **image** available at the URL given, exposed at **port 8080** on the stack/swarm/cluster/from the outside world and **port 8080** on the container hosting the image replica. It accesses the **host's docker socket file on its volume**, will be deployed **only on the manager node** and will use the **webnet network policy**.
2. A **Redis database service** from the **image** at the URL specified, exposed at **port 6379** on the stack/swarm/cluster/from the outside world and **port 6379** on the container hosting the image replica. It accesses the **host's volume folder *``/home/docker/data``*** mapped to its *internal folder ``/data``* (This allows the ***data to persist as long as the host machine is not destroyed***. The data will persist after the machine stops, reboots,...). The service will be deployed only on the **manager node** and will use the **webnet network policy**.


**Note**:
  The Redis service always runs on the manager node meaning data it **always** uses the **same** file system. Also, it accesses an **arbitrary** folder on the host **machine** guaranteeing that if nothing else will fiddle with the data there (which should be the case) that data will persist after stop, shutdown restarts of the containers and the machine itself. These 2 facts makes the database a **source of truth** that can be trusted to store the data for our stack/application


### Create the "/home/docker/data" folder on the manager node

```
docker-machine ssh myvm1 "mkdir ./data"
```

### Redeploy the app

Configure current terminal to talk directly to the node
```
docker-machine env myvm1
```

Redeploy the app
```
docker stack deploy -c docker-compose.yml getstartedlab
```

Verify the statuses of the 3 services
```
docker service ls
```

At this point a visit from the browser at:
```
http://192.168.8.104/
http://192.168.8.104:8080/
```
will hit our app's end point and the visualizer's service endpoint. 


The visualizer confirms that:
  3 replicas of our get-started:part2 image are running on the worker. 
  2 replicas of our get-started:part2 image, 1 instance of the visualizer and 1 instance of the redis database on the node manager

**Note**: 
  For some reason ``http://192.168.8.104`` works without the need to append the ingress port. 
  The requests response are way faster (That's probably due to the fact that we were trying to access a non existent database)
  I always hit the same container on the worker machine for every request (I am inclined to attribute that to the fact that the redis database was non existent before). This also says that for normal request volumes only one container is needed, and that the other ones do nothing until the request volumes force them to start servicing them
